{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Author and date\n",
    "Kristina Matiukhina, 20-Feb-2020\n",
    "\n",
    "#### Project Summary\n",
    "The purpose of the data engineering capstone project is to give students a chance to combine what they've learned throughout the program.\n",
    "\n",
    "In this project, students can choose to complete the project provided for them, or define the scope and data for a project of their own design. Either way, they'll be expected to go through the same steps outlined below.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import configparser\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession, types as T\n",
    "from pyspark.sql.functions import udf, to_date, split, year, length\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The analytics team wants to know insights about:\n",
    "- What are top visa type, type of landing (land, sea, etc), entry point (port) and city and how they change over months?\n",
    "\n",
    "To answer this question we will use Spark to explore, clean and aggregate data to answer this question. Ideally, information is going to be analyzed every year (this is how often a new report is posted on the US National Travel and Tourism Office website) and appeneded to the monthly_analysis file in the analytics folder. Data is not partitioned and just appended, because we don't expext this file growing enourmously - every year it will add 12 new records with 6 columns - month, year, visa type, type of landing, entry point, city.\n",
    "\n",
    "\n",
    "#### Describe and Gather Data \n",
    "US government stores information about visitors. Information contains high-level description of the visitor like age, gender, visa type or occupation they will have in US. It also contains the entry point and type of travelling they used to land into the country, and updated information about when the visitor left. This information is provided on US National Travel and Tourism Office website https://travel.trade.gov.\n",
    "\n",
    "In addition, we have lists of all CBP codes represented by 3 letters and corresponding city and state. The file is created from https://redbus2us.com/travel/usa/us-customs-and-border-protection-cbp-codes-port-of-entry-stamp/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(cicid=6.0, i94yr=2016.0, i94mon=4.0, i94cit=692.0, i94res=692.0, i94port='XXX', arrdate=20573.0, i94mode=None, i94addr=None, depdate=None, i94bir=37.0, i94visa=2.0, count=1.0, dtadfile=None, visapost=None, occup=None, entdepa='T', entdepd=None, entdepu='U', matflag=None, biryear=1979.0, dtaddto='10282016', gender=None, insnum=None, airline=None, admnum=1897628485.0, fltno=None, visatype='B2')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "    .enableHiveSupport().getOrCreate()\n",
    "df_spark =spark.read.format('com.github.saurfang.sas.spark').\\\n",
    "    load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "df_spark.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "##### US immigration data\n",
    "This set has not very intuitive column names. It also has values which heavily depend on data dictionary provided with the set - I94_SAS_Labels_Descriptions. \n",
    "\n",
    "Arrdate is in sas format, so you use it we should convert it to days and add to 01-01-1960. \n",
    "``` \n",
    "get_datestamp = udf(lambda x: pd.to_timedelta(x, unit='d') + pd.datetime(1960, 1, 1), T.DateType()) \n",
    "a = a.withColumn(\"arrival_date\", get_datestamp(a.arrdate))\n",
    "a.where(\"month(arrival_date) != month and year(arrival_date) != year\").take(5)\n",
    "```\n",
    "However, after exploration of data, there is no discrepency between i94yr and i94mon and month(arrdate) and year(arr_date). So, for our aggregation we can use i94yr and i95mon without complex manipulation of arrdate column.\n",
    "\n",
    "For some columns we will also need to use data dictionary - I94_SAS_Labels_Descriptions.sas - to translate short values into more informational ones. We are going to use it for i94mode and i94visa.\n",
    "\n",
    "For translating i94port, we will go to use CPBcodes.csv as a dictionary ans second source of data.\n",
    "\n",
    "##### CBP codes data\n",
    "This is pretty straight-forward data source. It contains CPB codes and location. City and state are stored in the same column \"location\", so we need to split them into two. Also some locations are empty or have more then 2 letter code for states, so we will need investigate and clean up.\n",
    "\n",
    "#### Cleaning Steps\n",
    "##### US immigration data\n",
    "Filter records with 'not reported' mode or with value 9.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark = df_spark.filter(df_spark[\"i94mode\"] != 9.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Select columns required for aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immig_df = df_spark.selectExpr(\"int(cicid) as id\", \"int(i94yr) as year\", \\\n",
    "    \"int(i94mon) as month\", \"i94port as port\", \"int(i94mode) as i94mode\", \\\n",
    "    \"int(i94visa) as i94visa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Replace i94mode and i94visa with values from data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# replace i94mode data with values from dictionary\n",
    "get_landing_type = udf(lambda x: \n",
    "                       \"Air\" if x == 1 else\n",
    "                       \"Sea\" if x == 2 else\n",
    "                       \"Land\" if x == 3 else\n",
    "                       \"Not reported\", T.StringType()) \n",
    "immig_df = immig_df.withColumn(\"landing_type\", \n",
    "                               get_landing_type(immig_df.i94mode))\n",
    "\n",
    "# replace i94visa data with values from dictionary\n",
    "get_visa_type = udf(lambda x: \n",
    "                       \"Business\" if x == 1 else\n",
    "                       \"Pleasure\" if x == 2 else\n",
    "                       \"Student\" if x == 3 else\n",
    "                       \"Not reported\", T.StringType()) \n",
    "immig_df = immig_df.withColumn(\"visa_type\", \n",
    "                               get_visa_type(immig_df.i94visa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=7, year=2016, month=4, port='ATL', landing_type='Air', visa_type='Student'),\n",
       " Row(id=15, year=2016, month=4, port='WAS', landing_type='Air', visa_type='Pleasure'),\n",
       " Row(id=16, year=2016, month=4, port='NYC', landing_type='Air', visa_type='Pleasure'),\n",
       " Row(id=17, year=2016, month=4, port='NYC', landing_type='Air', visa_type='Pleasure'),\n",
       " Row(id=18, year=2016, month=4, port='NYC', landing_type='Air', visa_type='Business'),\n",
       " Row(id=19, year=2016, month=4, port='NYC', landing_type='Air', visa_type='Pleasure'),\n",
       " Row(id=20, year=2016, month=4, port='NYC', landing_type='Air', visa_type='Pleasure'),\n",
       " Row(id=21, year=2016, month=4, port='NYC', landing_type='Air', visa_type='Pleasure'),\n",
       " Row(id=22, year=2016, month=4, port='NYC', landing_type='Air', visa_type='Business'),\n",
       " Row(id=23, year=2016, month=4, port='NYC', landing_type='Air', visa_type='Pleasure')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immig_df = immig_df.selectExpr(\"id\", \"year\", \"month\", \"port\", \"landing_type\",\n",
    "                               \"visa_type\")\n",
    "immig_df.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Store data in parquet file partitioned by year and month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immig_df.write.partitionBy(\"year\", \"month\").\\\n",
    "    parquet(\"immigration_data/\", mode=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### CBP codes data\n",
    "Read csv data and filter codes which expired before considered year (in this project it is 2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Code='ABE', Location='Aberdeen, WA'),\n",
       " Row(Code='ABQ', Location='Albuquerque, NM'),\n",
       " Row(Code='ADT', Location='Amistad Dam, TX'),\n",
       " Row(Code='ALP', Location='Alpena, MI'),\n",
       " Row(Code='AGN', Location='Algonac, MI'),\n",
       " Row(Code='AKR', Location='Akron, OH')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbp_df = spark.read.load(\"CBP_codes.csv\", format=\"csv\", sep=\",\", header=\"true\")\n",
    "cbp_df.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Split \"Location\" into \"city\" and \"state_code\" and delete all records which  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "split_col = split(cbp_df['Location'], ', ')\n",
    "cbp_df = cbp_df.withColumn('city', split_col.getItem(0))\n",
    "cbp_df = cbp_df.withColumn('state_code', split_col.getItem(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Find all state codes which didn't clean properly - lenght is not 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>state_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Saipan Island</td>\n",
       "      <td>Saipan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Agana</td>\n",
       "      <td>Guam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            city state_code\n",
       "0  Saipan Island     Saipan\n",
       "1          Agana       Guam"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = cbp_df.withColumn(\"ln\", length(\"state_code\"))\n",
    "a.select(\"city\", \"state_code\").where(\"ln != 2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Clean 42 records which we found in previous step and, just in case, all canadian states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "clean_states = udf(lambda x: \\\n",
    "    \"TX\" if x == \"B&M Bridge\" \n",
    "                   or x == \"Fort Duncan Bridge\" \n",
    "                   or x == \"TX Bridge of Americas\" \n",
    "                   or x == \"Gateway Bridge\" \n",
    "                   or x == \"TX Juarez-Lincoln Bridge\" \n",
    "                   or x == \"Columbia Bridge\" \n",
    "                   or x == \"World Trade Bridge\" \n",
    "                   or x == \"El Paso\" \n",
    "    else \"ME\" if x == \"ME Ferry Terminal\" \n",
    "    else \"VI\" if x == \"St. Thomas\" \n",
    "                   or x == \"St. Croix\" \n",
    "                   or x == \"St. John\" \n",
    "                   or x == \"USVI\" \n",
    "    else None if x == \"Ireland\" \n",
    "                   or x == \"AFB DE\" \n",
    "                   or x == \"AB Canada\" or x == \"Alberta Canada\" or x == \"AB\"\n",
    "                   or x == \"Nova Scotia\" or x == \"NS\" or x == \"NL\"\n",
    "                   or x == \"Quebec\" or x == \"QC\"\n",
    "                   or x == \"Canada\" or x == \"NB\" or x == \"MB\" or x == \"PE\"  \n",
    "                   or x == \"Island\" \n",
    "                   or x == \"SK. Canada\" or x == \"SK\"\n",
    "                   or x == \"IAP at Edinburg\" \n",
    "                   or x == \"Bahamas\" \n",
    "                   or x == \"Bermuda\" \n",
    "                   or x == \"Italy\" \n",
    "                   or x == \"Ontario Canada\" or x == \"ON\" \n",
    "                   or x == \"BC Canada\" or x == \"BC\" \n",
    "    else \"MI\" if x == \"Ambassador Bridge\" \n",
    "                   or x == \"M\" \n",
    "                   or x == \"Windsor Tunnel\" \n",
    "                   or x == \"MI Kent County Intl Airport\" \n",
    "    else \"Saipan\" if x == \"Saipan Intl Arpt\" \n",
    "    else \"CA\" if x == \"Truck Crossing\" \n",
    "    else \"NY\" if x == \"Rainbow Bridge\" \n",
    "    else \"DC\" if x == \"Washington DC\" \n",
    "    else \"ND\" if x == \"ND (Int'l Airport)\" \n",
    "    else x, T.StringType())\n",
    "cbp_df = cbp_df.withColumn(\"state_code\", clean_states(cbp_df.state_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Filter records with null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "cbp_df = cbp_df.filter(cbp_df['state_code'] != \"None\")\n",
    "\n",
    "cbp_df = cbp_df.select('code','city','state_code').dropDuplicates([\"code\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Store cleaned file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "cbp_df.toPandas().to_csv(r'./port_codes/port_codes.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Data Model is very simple, as it was created to answer only the question which analytics asked for.\n",
    "##### Staging-like tables\n",
    "Those data are required for aggregations to deliver response about data to analytics.\n",
    "- 1 dimension table which stores port_codes \n",
    "port_code |city  |state\n",
    "string, PK|string|string\n",
    "Data stored in port_codes folder.\n",
    "- 1 fact table which stored cleaned immigration data ready for aggregation\n",
    "id     |year|month|port  |landing_type|visa_type\n",
    "int, PK|int |int  |string|string      |string\n",
    "Data stored in immigration_data folder.\n",
    "\n",
    "##### Result table\n",
    "As analytics team asked for top visa_type, landing_type, port and associated to it city for each month each year. We can have one more result table or view with the following columns:\n",
    "- year\n",
    "- month\n",
    "- top_visa_type \n",
    "- top_lading_type\n",
    "- port - the one which entered mostly\n",
    "- city - associated to the most entered port\n",
    "As mentioned earlier, data will be processed ones a year and will do aggregations for new 12 month only. So, we will just append data into the file, instead of overwritting it. \n",
    "Data are stored in analytics folder.\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "Steps:\n",
    "1. Read data from immigration_data folder for a new year\n",
    "2. Read data from port_codes\n",
    "3. Join both sets on immigration_data.port and port_codes.port_code. It should be inner join to get rid off records in immigration table which does not have a valid port in US.\n",
    "4. Run a few separate aggregations for each month - one to identify top visa, another to identify top landing type, and last one to identify top \n",
    "5. Merge all 3 aggregations into one row with month and year\n",
    "6. Append new date to existing file in analytics folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "    config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "    .enableHiveSupport().getOrCreate()\n",
    "\n",
    "# set what year is picked up\n",
    "cur_year = 2016\n",
    "immig_filepath = 'immigration_data/year=' + str(cur_year)\n",
    "# read both required data sets\n",
    "immig_df = spark.read.parquet(immig_filepath)\n",
    "p_df = spark.read.load(\"./port_codes/port_codes.csv\", format=\"csv\", \n",
    "                       sep=\",\", header=\"true\")\n",
    "\n",
    "# do inner join on port_code\n",
    "res_df = immig_df.join(p_df, immig_df.port == p_df.code, how='inner')\n",
    "\n",
    "# aggregations\n",
    "top_visa = res_df.groupBy('month','visa_type').count().\\\n",
    "    orderBy('count', ascending=False).limit(1).select('month','visa_type')\n",
    "top_landing = res_df.groupBy('month','landing_type').count().\\\n",
    "    orderBy('count', ascending=False).limit(1).select('month','landing_type')\n",
    "top_port = res_df.groupBy('month','port', 'city').count().\\\n",
    "    orderBy('count', ascending=False).limit(1).select('month','port', 'city')\n",
    "\n",
    "# join all aggregations together and add year column\n",
    "year_expr = 'int(' + str(cur_year) + ') as year'\n",
    "agg_df = top_visa.join(top_landing, on = ['month'], how='inner').\\\n",
    "    join(top_port, on = ['month'], how='inner').selectExpr(year_expr, \\\n",
    "    \"month\", \"visa_type\",\"landing_type\", \"port\", \"city\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# append data to analytics file\n",
    "if not os.path.isfile('analytics.csv'):\n",
    "    agg_df.toPandas().to_csv(r'analytics.csv', header=True)\n",
    "else: # else it exists so append without writing the header\n",
    "    agg_df.toPandas().to_csv(r'analytics.csv', mode=\"a\", header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test1: Codes from port codes file should be all unique\n",
      "Test1 is passed\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "    config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "    .enableHiveSupport().getOrCreate()\n",
    "\n",
    "# Uniqueness of codes in port codes is highly important, \n",
    "# especially because we inner join those codes with immigration data\n",
    "# We don't want results for a duplicate code being calculated twice, \n",
    "# as analytics might get th wrong top information\n",
    "print(\"Test1: Codes from port codes file should be all unique\")\n",
    "p_df = spark.read.load(\"./port_codes/port_codes.csv\", format=\"csv\",\n",
    "                       sep=\",\", header=\"true\")\n",
    "if p_df.count() <= 0:\n",
    "    raise Exception(\"Test1 is failed: there is no data read from port_codes\")\n",
    "elif p_df.count() != p_df.select('code').distinct().count():\n",
    "    raise Exception(\"Test1 is failed: there are duplicate codes in port_codes\")\n",
    "else:\n",
    "    print(\"Test1 is passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test2: Analytics file should have 1 record for each month and year pair\n",
      "Test2 is passed\n"
     ]
    }
   ],
   "source": [
    "print(\"Test2: Analytics file should have 1 record for each month and year pair\")\n",
    "p_df = spark.read.load(\"./analytics.csv\", format=\"csv\", sep=\",\", header=\"true\")\n",
    "if p_df.count() <= 0:\n",
    "    raise Exception(\"Test2 is failed: there is no data read from analytics.csv\")\n",
    "elif p_df.groupBy('year','month').count().select('count').filter('count > 1').\\\n",
    "    count() != 0:\n",
    "    raise Exception(\"Test2 is failed: there are duplicate month and year\" +\n",
    "                    \" combinations in analytics.csv\")\n",
    "else:\n",
    "    print(\"Test2 is passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test3: Analytics file should not have empty values\n",
      "Test3 is passed\n"
     ]
    }
   ],
   "source": [
    "print(\"Test3: Analytics file should not have empty values\")\n",
    "p_df = spark.read.load(\"./analytics.csv\", format=\"csv\", sep=\",\", header=\"true\")\n",
    "if p_df.filter(p_df.year.isNull()).count() > 0 \\\n",
    "    or p_df.filter(p_df.month.isNull()).count() > 0 \\\n",
    "    or p_df.filter(p_df.visa_type.isNull()).count() > 0 \\\n",
    "    or p_df.filter(p_df.landing_type.isNull()).count() > 0 \\\n",
    "    or p_df.filter(p_df.port.isNull()).count() > 0 \\\n",
    "    or p_df.filter(p_df.city.isNull()).count() > 0:\n",
    "        raise Exception(\"Test3 is failed: there is empty values in analytics.csv\")\n",
    "else:\n",
    "    print(\"Test3 is passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "##### Immigration Data\n",
    "Stored in immigration_data folder and partitioned by year and then month. \n",
    "\n",
    "***id***\n",
    "\n",
    "this is a unique integer value for every immigration record. It originally comes from uncleaned data set loaded from US National Travel and Tourism Office website (https://travel.trade.gov).\n",
    "\n",
    "***year***\n",
    "\n",
    "4 digit year\n",
    "\n",
    "***month*** \n",
    "\n",
    "numeric month\n",
    "\n",
    "***port***\n",
    "\n",
    "3 letter abbreviation for the entry port name\n",
    "\n",
    "***landing_type***\n",
    "\n",
    "the entry way which was used to entry US.\n",
    "\n",
    "Allowed values are: *Air, Sea, Land*\n",
    "\n",
    "***visa_type*** \n",
    "\n",
    "category of visa used to entry US. \n",
    "\n",
    "Allowed values: *Business, Pleasure, Student*\n",
    "\n",
    "\n",
    "##### Port Data\n",
    "Stored only US entry port information. It is represented in csv format and located in port_codes foder. \n",
    "\n",
    "port_code |city  |state\n",
    "\n",
    "***port_code***\n",
    "\n",
    "uniqque 3 letter abbreviation for the entry port name\n",
    "\n",
    "***city***\n",
    "\n",
    "city associated to entry port\n",
    "\n",
    "***state*** \n",
    "\n",
    "2 letter state or territory abbreviation associated to entry port.\n",
    "\n",
    "Exceptions are: *Guam, Saipan*\n",
    "\n",
    "\n",
    "##### Analytics Data\n",
    "Provides analytics with required insights about the most common characteristics of entry US. Data calculated for each month each year.\n",
    "It is located in analytics.csv.\n",
    "\n",
    "***year***\n",
    "\n",
    "4 digit year\n",
    "\n",
    "***month*** \n",
    "\n",
    "numeric month\n",
    "\n",
    "***visa_type*** \n",
    "\n",
    "category of visa used motsly to entry US within the certain month and year\n",
    "\n",
    "Allowed values: *Business, Pleasure, Student*\n",
    "\n",
    "***landing_type***\n",
    "\n",
    "the entry way which was used montsly to entry US within the certain month and year\n",
    "\n",
    "Allowed values are: *Air, Sea, Land*\n",
    "\n",
    "***port***\n",
    "\n",
    "3 letter abbreviation for the entry port name that was used mostly to enter US within the certain month and year\n",
    "  \n",
    "***city***\n",
    "\n",
    "city associated to entry port"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "1. As the required data for analytics is very small and will be growing maximum 12 lines per year, we can use csv format which widely used by analytics teams in various companies. \n",
    "Because of the same reason, I don't see the point of creating databases or warehouses to store this information. Data can be easily cleaned and stored as backup in folders, but they are going to be used ones for this analytics. New set od data will be analized every year and append to xisting csv. Spark is fast enough tool to consider 1 year worth if data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "2. I would suggest creating a scheduler which will pick up a new file for immigration data and run all steps ones a year, as US government provides this data only once a year for a previous year. It would be much better to automate this process to avoid running etls on the same year twice or more and as a result appending duplicate information to analytics file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "3.1 If data increases by 100x, then there 2 things I would change:\n",
    "- store immigration data in parquet with partition by month and year right away and then clean each set separately\n",
    "- do analytical aggregations for month worth data and not whole year at the same time\n",
    "- Note: that eventually it might be needed to scale spark instances and add nodes.\n",
    "\n",
    "3.2 This question is not relavant to my solution\n",
    "\n",
    "3.3 It is possible to create a small view stored in postgres or redshift for people to access analytics data and make sure noone changes anything. So, substitute csv to view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
